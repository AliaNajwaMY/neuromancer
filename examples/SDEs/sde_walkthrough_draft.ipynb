{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from neuromancer.system import Node, System\n",
    "from neuromancer.dynamics import integrators, ode\n",
    "from neuromancer.trainer import Trainer\n",
    "from neuromancer.problem import Problem\n",
    "from neuromancer.dataset import DictDataset\n",
    "from neuromancer.constraint import variable\n",
    "from neuromancer.loss import PenaltyLoss\n",
    "from neuromancer.modules import blocks\n",
    "from neuromancer.psl import plot\n",
    "from neuromancer import psl\n",
    "\n",
    "\n",
    "from typing import Sequence\n",
    "import abc\n",
    "import torchsde\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "An ordinary differential equation is given by \n",
    "\n",
    "The general form of an ordinary differential equation (ODE) is:\n",
    "\n",
    "$$ \\frac{{dx}}{{dt}} = f(t, x) $$\n",
    "\n",
    "A neural ordinary differential equation replaces the RHS with a neural network. That is, the evolution of a system over time is represented by a continuous flow governed by an ODE, where the dynamics are parameterized by a neural network, as shown below: \n",
    "\n",
    "a continuous-time NODE model: $\\dot{x} = f_{\\theta}(x)$ with trainable parameters $\\theta$.\n",
    "\n",
    "Given training data consisting of several time-series \"episode\" (e.g. the system dynamics at t, t+1, t+2 -- which in Neuromancer terminology would be a rollout of nsteps=2), we can train such Neural ODE: \n",
    "\n",
    "Next we need to solve the continuous-time NODE model with suitable ODE solver, e.g., [Runge–Kutta integrator](https://en.wikipedia.org/wiki/Runge%E2%80%93Kutta_methods).  \n",
    "$x_{k+1} = \\text{ODESolve}(f_{\\theta}(x_k))$ \n",
    "\n",
    "For training we need to obtain accurate reverse-mode gradients of the integrated NODE system. This can be done in two ways, either by unrolling the operations of the ODE solver and using the [backpropagation through time](https://en.wikipedia.org/wiki/Backpropagation_through_time) (BPTT) algorithm, or via [Adjoint state method](https://en.wikipedia.org/wiki/Adjoint_state_method).\n",
    "\n",
    "Schematics illustrating the adjoing method used in the [Neural Ordinary Differential Equations](https://arxiv.org/abs/1806.07366) paper:\n",
    "<img src=\"../figs/NODE_backprop.png\" width=\"500\">  \n",
    "\n",
    "Neuromancer provides a set of ODE solvers implemented in [integrators.py](https://github.com/pnnl/neuromancer/blob/master/src/neuromancer/dynamics/integrators.py).\n",
    "For adjoint method we provide the interface to the [open-source implementation](https://github.com/rtqichen/torchdiffeq) via DiffEqIntegrator class.\n",
    "\n",
    "We give a quick example here to motivate how one might include (and not include) stochasticity (randomness) into the system dynamics. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ordinary Lotka-Volterra system, also known as the predator-prey model, describes the dynamics of two interacting species in a biological community. The system consists of two coupled ordinary differential equations (ODEs), typically represented as follows:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{dX}{dt} &= (\\alpha X - \\beta XY) dt  \\\\\n",
    "\\frac{dY}{dt} &= (\\delta XY - \\gamma Y) dt\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- \\(X\\) and \\(Y\\) represent the population sizes of the prey and predator species, respectively.\n",
    "- \\(\\alpha\\), \\(\\beta\\), \\(\\gamma\\), and \\(\\delta\\) are parameters governing the growth and interaction rates of the species.\n",
    "- \\(dW_1\\) and \\(dW_2\\) are independent Wiener processes representing white noise in the population dynamics.\n",
    "- \\(\\sigma_1\\) and \\(\\sigma_2\\) are the volatility parameters associated with the noise processes.\n",
    "\n",
    "This system captures the stochastic fluctuations in population sizes due to random environmental factors, which can influence the dynamics of predator-prey interactions over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a neural network, denoted as $f_{\\theta}$ which represents the right-hand side (RHS) of the ordinary differential equation (ODE). This neural network takes the current state of the system as input and outputs the rate of change (derivative) of the state variables. \n",
    "\n",
    "We then use ODE solver to generate data states at future time, e.g. at $t+1$ given by $x_{k+1} = \\text{ODESolve}(f_{\\theta}(x_k))$ \n",
    "\n",
    "\n",
    "You train the entire model, including the neural network dynamics and the ODE solver, end-to-end using pairs of consecutive time points (for the case of a rollout of 1 step, though this can be scaled up to predict longer horizons) from your dataset. We reshape our full system trajectory dataset into these bunched time \"episodes\" to achieve this rollout-based training. \n",
    "\n",
    "​"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LotkaVolterraHybrid(ode.ODESystem):\n",
    "\n",
    "    def __init__(self, block, insize=2, outsize=2):\n",
    "        \"\"\"\n",
    "\n",
    "        :param block:\n",
    "        :param insize:\n",
    "        :param outsize:\n",
    "        \"\"\"\n",
    "        super().__init__(insize=insize, outsize=outsize)\n",
    "        self.block = block\n",
    "        self.alpha = nn.Parameter(torch.tensor([.10]), requires_grad=True)\n",
    "        self.beta = nn.Parameter(torch.tensor([.10]), requires_grad=True)\n",
    "        self.delta = nn.Parameter(torch.tensor([.10]), requires_grad=True)\n",
    "        self.gamma = nn.Parameter(torch.tensor([.10]), requires_grad=True)\n",
    "        assert self.block.in_features == 2\n",
    "        assert self.block.out_features == 1\n",
    "\n",
    "    def ode_equations(self, x):\n",
    "        x1 = x[:, [0]]\n",
    "        x2 = x[:, [-1]]\n",
    "        dx1 = self.alpha*x1 - self.beta*self.block(x)\n",
    "        dx2 = self.delta*self.block(x) - self.gamma*x2\n",
    "        return torch.cat([dx1, dx2], dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\dot{x} = f_{\\vec{\\theta}}(x)$ \n",
    "\n",
    "Here, $  \\vec{\\theta} $ is a vector with parameters  $ [\\alpha, \\beta, \\gamma, \\delta, \\theta'] $ and $\\theta'$ parameterizes the multi-layer perception block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(sys, nsim, nsteps, ts, bs):\n",
    "    \"\"\"\n",
    "    :param nsteps: (int) Number of timesteps for each batch of training data\n",
    "    :param sys: (psl.system)\n",
    "    :param ts: (float) step size\n",
    "    :param bs: (int) batch size\n",
    "\n",
    "    \"\"\"\n",
    "    train_sim, dev_sim, test_sim = [sys.simulate(nsim=nsim, ts=ts) for i in range(3)]\n",
    "    nx = sys.nx\n",
    "    nbatch = nsim//nsteps #500\n",
    "    length = (nsim//nsteps) * nsteps #1000\n",
    "    ts = torch.linspace(0,1,nsteps)\n",
    "    print('train sim ', train_sim['X'].shape)\n",
    "\n",
    "    trainX = train_sim['X'][:length].reshape(nbatch, nsteps, nx)\n",
    "    trainX = torch.tensor(trainX, dtype=torch.float32)\n",
    "\n",
    "    print(trainX.shape)# N x nsteps x state_size \n",
    "\n",
    "    train_data = DictDataset({'X': trainX, 'xn': trainX[:, 0:1, :]}, name='train')\n",
    "    train_loader = DataLoader(train_data, batch_size=bs,\n",
    "                              collate_fn=train_data.collate_fn, shuffle=True)\n",
    "\n",
    "    devX = dev_sim['X'][:length].reshape(nbatch, nsteps, nx)\n",
    "    devX = torch.tensor(devX, dtype=torch.float32)\n",
    "    dev_data = DictDataset({'X': devX, 'xn': devX[:, 0:1, :]}, name='dev')\n",
    "    dev_loader = DataLoader(dev_data, batch_size=bs,\n",
    "                            collate_fn=dev_data.collate_fn, shuffle=True)\n",
    "\n",
    "    testX = test_sim['X'][:length].reshape(1, nsim, nx)\n",
    "    testX = torch.tensor(testX, dtype=torch.float32)\n",
    "    test_data = {'X': testX, 'xn': testX[:, 0:1, :]}\n",
    "\n",
    "    return train_loader, dev_loader, test_data, trainX\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# %%  ground truth system\n",
    "system = psl.systems['LotkaVolterra']\n",
    "modelSystem = system()\n",
    "ts = modelSystem.ts\n",
    "nx = modelSystem.nx\n",
    "raw = modelSystem.simulate(nsim=1000, ts=ts)\n",
    "plot.pltOL(Y=raw['X'])\n",
    "plot.pltPhase(X=raw['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural ODEs with Added Stochasticity: \n",
    "\n",
    "A stochastic differential equation is given by :\n",
    "\n",
    "$$ dx = f(t, x) \\, dt + g(t, x) \\, dW $$\n",
    "\n",
    "THe $f$ term is known as the drift process; the $g$ term is known as the diffusion process. Note that if the diffusion process is zero then an SDE simplifies to an ODE and can be solved with via backpropagating ODE solver and doing the reverse-time ODE as we have shown previously. \n",
    "\n",
    "For simplicity we can assume there exists reverse-time integration/backpropagating through SDE solvers. This paper, https://arxiv.org/pdf/2001.01328, describes it in detail. \n",
    "\n",
    "A natural question therefore is how to train such Neural ODEs with Stochastic Terms. Can we do it using standard Neuromancer training procedure for Neural ODEs -- our reference tracking and finite difference losses. We attempt to do this below. Note that it will **not** work and the purpose of demonstrating this is to motivate the need for a variational inference approach to train the SDE -- the Latent SDE. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can formulate neural networks to parameterize not only the drift process, but also the diffusion process, e.g: \n",
    "\n",
    "$$ \\dot{x} = f_{\\vec{\\theta_f}}(x) + g_{\\vec{\\theta_g}}(x) $$\n",
    "\n",
    "Where $g$ is a neural network to model the stochastic process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To support this framework and integrate it with TorchSDE solvers, we define a base class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseSDESystem(abc.ABC, nn.Module):\n",
    "    \"\"\"\n",
    "    Base class for SDEs for integration with TorchSDE library\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.noise_type = \"diagonal\"\n",
    "        self.sde_type = \"ito\"\n",
    "        self.in_features = 0\n",
    "        self.out_features = 0\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def f(self, t, y):\n",
    "        \"\"\"\n",
    "        Define the ordinary differential equations (ODEs) for the system.\n",
    "\n",
    "        Args:\n",
    "            t (Tensor): The current time (often unused)\n",
    "            y (Tensor): The current state variables of the system.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The derivatives of the state variables with respect to time.\n",
    "                    The output should be of shape [batch size x state size]\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def g(self, t,y):\n",
    "        \"\"\"\n",
    "        Define the diffusion equations for the system.\n",
    "\n",
    "        Args:\n",
    "            t (Tensor): The current time (often unused)\n",
    "            y (Tensor): The current state variables of the system.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The diffusion coefficients per batch item (output is of size \n",
    "                    [batch size x state size]) for noise_type 'diagonal'\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a stochastic Lotka-Volterra model (for forward passes only) with user-defined parameters to generate ground truth data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Lotka-Volterra SDE\n",
    "class LotkaVolterraSDE(nn.Module):\n",
    "    def __init__(self, a, b, c, d, sigma1, sigma2):\n",
    "        super().__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "        self.d = d\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "        self.noise_type = \"diagonal\"\n",
    "        self.sde_type = \"ito\"\n",
    "\n",
    "    def f(self, t, x):\n",
    "        x1 = x[:,[0]]\n",
    "        x2 = x[:,[1]]\n",
    "        dx1 = self.a * x1 - self.b * x1*x2\n",
    "        dx2 = self.c * x1*x2 - self.d * x2\n",
    "        foo = torch.cat([dx1, dx2], dim=-1)\n",
    "        return torch.cat([dx1, dx2], dim=-1)\n",
    "\n",
    "    def g(self, t, x):\n",
    "        sigma_diag = torch.tensor([[self.sigma1, self.sigma2]])\n",
    "        return sigma_diag #[batch_size x state size ]\n",
    "\n",
    "# Define parameters\n",
    "a = 1.1    # Prey growth rate\n",
    "b = 0.4   # Predation rate\n",
    "c = 0.1   # Predator growth rate\n",
    "d = 0.4   # Predator death rate\n",
    "sigma1 = 1\n",
    "sigma2 = 0\n",
    "\n",
    "# Create the SDE model\n",
    "sde = LotkaVolterraSDE(a, b, c, d, sigma1, sigma2)\n",
    "\n",
    "\n",
    "# Define time span\n",
    "t_span = torch.linspace(0, 20, 2000)\n",
    "\n",
    "# Initial condition\n",
    "x0 = torch.tensor([10.0, 10.0]).unsqueeze(0) #[1x2]\n",
    "\n",
    "\n",
    "# Integrate the SDE model\n",
    "sol_train = torchsde.sdeint(sde, x0, t_span, method='euler')\n",
    "sol_dev = torchsde.sdeint(sde, x0, t_span, method='euler')\n",
    "sol_test = torchsde.sdeint(sde, x0, t_span, method='euler')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(t_span, sol_train[:, 0,0], label='Prey (x1)')\n",
    "plt.plot(t_span, sol_train[:,0, 1], label='Predator (x2)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Population')\n",
    "plt.title('Stochastic Lotka-Volterra Predator-Prey Model')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LotkaVolterraSDELearnable(BaseSDESystem):\n",
    "    def __init__(self, block, batch_size):\n",
    "        super().__init__()\n",
    "        self.block = block \n",
    "        self.alpha = nn.Parameter(torch.tensor([.10]), requires_grad=True)\n",
    "        self.beta = nn.Parameter(torch.tensor([.10]), requires_grad=True)\n",
    "        self.delta = nn.Parameter(torch.tensor([.10]), requires_grad=True)\n",
    "        self.gamma = nn.Parameter(torch.tensor([.10]), requires_grad=True)\n",
    "        self.g_params = nn.Parameter(torch.randn(batch_size, 2), requires_grad=True)  # Learnable parameters\n",
    "    def f(self, t, y):\n",
    "\n",
    "        x1 = y[:, [0]]\n",
    "        x2 = y[:, [-1]]\n",
    "\n",
    "        dx1 = self.alpha*x1 - self.beta*self.block(y)\n",
    "        dx2 = self.delta*self.block(y) - self.gamma*x2\n",
    "\n",
    "        return torch.cat([dx1, dx2], dim=-1)\n",
    "\n",
    "    def g(self, t, y):\n",
    "        return self.g_params\n",
    "\n",
    "# construct UDE model in Neuromancer\n",
    "net = blocks.MLP(2, 1, bias=True,\n",
    "                    linear_map=torch.nn.Linear,\n",
    "                    nonlin=torch.nn.GELU,\n",
    "                    hsizes=4*[20])\n",
    "fx = LotkaVolterraSDELearnable(block=net, batch_size = 2)\n",
    "\n",
    "\n",
    "class BasicSDEIntegrator(integrators.Integrator): \n",
    "    \"\"\"\n",
    "    Integrator (from TorchSDE) for basic/explicit SDE case where drift (f) and diffusion (g) terms are defined \n",
    "    Returns a single tensor of size (t, batch_size, state_size).\n",
    "\n",
    "    Please see https://github.com/google-research/torchsde/blob/master/torchsde/_core/sdeint.py\n",
    "    Currently only supports Euler integration. Choice of integration method is dependent \n",
    "    on integral type (Ito/Stratanovich) and drift/diffusion terms\n",
    "    \"\"\"\n",
    "    def __init__(self, block ): \n",
    "        \"\"\"\n",
    "        :param block: (nn.Module) The BasicSDE block\n",
    "        \"\"\"\n",
    "        super().__init__(block) \n",
    "\n",
    "\n",
    "    def integrate(self, x): \n",
    "        \"\"\"\n",
    "        x is the initial datastate of size (batch_size, state_size)\n",
    "        t is the time-step vector over which to integrate\n",
    "        \"\"\"\n",
    "        t = torch.tensor([0.,0.01, 0.02], dtype=torch.float32)\n",
    "        x = x.squeeze(1) #remove time step \n",
    "  \n",
    "        ys = torchsde.sdeint(self.block, x, t, method='euler')\n",
    "        ys = ys.permute(1, 0, 2)\n",
    "        return ys \n",
    "\n",
    "integrator = BasicSDEIntegrator(fx) \n",
    "# integrate UDE model\n",
    "# create symbolic UDE model\n",
    "model_sde = Node(integrator, input_keys=['xn'], output_keys=['xn'])\n",
    "dynamics_model_sde = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using mean squared error (MSE) to train a neural SDE on time pairs might encounter challenges due to the stochastic nature of SDEs. While MSE is a common loss function used for deterministic systems, it may not be directly applicable to stochastic systems like SDEs.\n",
    "\n",
    "Here are some considerations when using MSE for training neural SDEs:\n",
    "\n",
    "Ignoring Stochasticity: MSE only considers the deterministic part of the model and ignores the stochastic component represented by the diffusion term in the SDE. This can lead to suboptimal results as the model does not capture the inherent randomness in the system.\n",
    "\n",
    "Overfitting the Drift Term: MSE optimization might focus excessively on minimizing the errors in the drift term while neglecting the diffusion term. This can result in overfitting of the deterministic part of the model and underfitting of the stochastic part.\n",
    "\n",
    "SDEs inherently involve randomness or uncertainty, typically represented by the stochastic terms in the differential equations. Variational inference allows us to capture this uncertainty by providing a probabilistic characterization of the latent variables' distribution. Instead of obtaining a single point estimate, variational inference provides a full probabilistic description, including measures of uncertainty such as confidence intervals or predictive distributions. \n",
    "\n",
    "The Latent SDE is essentially a variational \"autoencoder\" where instead of seeking resynthesize samples corresponding to the \"same time\" instance, it tries to reconstruct future samples given the current dynamics of the system, where the dynamics are known to be modeled via a SDE. To do this, the latent space is **itself** going to be governed via an SDE and we perform integration on the latent space to synthesize forward-looking samples. \n",
    "\n",
    "Variational inference is a powerful method used to approximate complex posterior distributions in probabilistic models. In the context of latent stochastic differential equations (SDEs), variational inference plays a crucial role in estimating the posterior distribution of the latent variables given the observed data.\n",
    "\n",
    "In latent SDEs, the goal is to infer the hidden or latent variables that govern the dynamics of the system. These latent variables capture unobserved factors that influence the observed data, such as underlying trends, patterns, or noise sources. However, directly computing the posterior distribution of the latent variables given the data is often analytically intractable due to the complex and nonlinear nature of the model.\n",
    "\n",
    "Variational inference offers a solution to this problem by approximating the true posterior distribution with a simpler, parameterized distribution, often chosen from a family of distributions such as Gaussian distributions. This is done via an encoder network. The decoder network draws from samples of this learned, approximate posterior to reconstruct the data distribution. Using the KL divergence, between these distributions, we learn the latent space's parameters known as the variational parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent SDE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Encoder**:\n",
    "   - The encoder maps each input data point $x$ to a distribution over latent variables $z$. This distribution is typically Gaussian with mean $\\mu$ and standard deviation $\\sigma$.\n",
    "   $$\n",
    "   q_{\\phi}(z | x) = \\mathcal{N}(\\mu_{\\phi}(x), \\sigma_{\\phi}(x))\n",
    "   $$\n",
    "   - Here, $\\mu_{\\phi}(x)$ and $\\sigma_{\\phi}(x)$ are the mean and standard deviation parameters of the Gaussian distribution, which are output by the encoder neural network parameterized by $\\phi$.\n",
    "\n",
    "2. **Latent Dynamics**:\n",
    "   - The latent variables $z$ evolve over time according to a stochastic differential equation (SDE). The dynamics of $z$ are governed by drift and diffusion functions, similar to the SDE for the observed data $x$.\n",
    "   $$\n",
    "   dz_t = f(z_t, t) \\, dt + G(z_t, t) \\, dW_t\n",
    "   $$\n",
    "   - $f(z_t, t)$ represents the drift component, determining the deterministic evolution of the latent variables.\n",
    "   - $G(z_t, t)$ represents the diffusion component, introducing stochasticity into the latent dynamics.\n",
    "   - $dW_t$ is the increment of a Wiener process (Brownian motion), representing random noise.\n",
    "\n",
    "3. **Decoder**:\n",
    "   - The decoder takes samples from the latent space $z$ and maps them back to the data space $x$. It models the conditional distribution of $x$ given $z$.\n",
    "   $$\n",
    "   p_{\\theta}(x | z)\n",
    "   $$\n",
    "   - The decoder neural network, parameterized by $\\theta$, outputs the parameters of the conditional distribution $p_{\\theta}(x | z)$, such as the mean and variance of a Gaussian distribution or the parameters of a Bernoulli distribution for binary data.\n",
    "\n",
    "4. **Latent Variable Prior**:\n",
    "   - We assume a prior distribution over the latent variables $z$. This distribution is typically chosen to be a standard Gaussian.\n",
    "   $$\n",
    "   p(z) = \\mathcal{N}(0, I)\n",
    "   $$\n",
    "\n",
    "   though in TorchSDE's framework (and as shown in the code below), these are learnable parameters qz0_mean and qz0\n",
    "\n",
    "5. **Objective Function**:\n",
    "   - The objective function for training the Latent SDE model is similar to the ELBO in VAEs but now includes the evolution of latent variables governed by the SDE.\n",
    "   $$\n",
    "   \\text{ELBO}(\\theta, \\phi; x) = \\mathbb{E}_{q_{\\phi}(z | x)} [\\log p_{\\theta}(x | z)] - \\text{KL}[q_{\\phi}(z | x) || p(z)]\n",
    "   $$\n",
    "   - The first term represents the reconstruction loss, measuring how well the decoder reconstructs the input data $x$ from the latent variable samples $z$.\n",
    "   - The second term is the KL divergence between the approximate posterior $q_{\\phi}(z | x)$ and the prior $p(z)$, which encourages the approximate posterior to match the prior.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
