{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchSDE + Neuromancer: Latent Stochastic Differential Equations (System ID of Stochastic Process)\n",
    "\n",
    "This notebook goes over how to utilize torchsde's functionality within Neuromancer framework. This notebook is based off: https://github.com/google-research/torchsde/blob/master/examples/latent_sde_lorenz.py. In this example, we generate data according to a 3-dimensional stochastic Lorenz attractor. We then perform a \"system identification\" on this data -- seek to model a stochastic differential equation on this data. Upon performant training, this LatentSDE will then be able to reproduce samples that exhibit the same behavior as the provided Lorenz system. We train and utilize the Lightning framework to support custom functionality within the training loop.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fd1303401d0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from neuromancer.psl import plot\n",
    "from neuromancer import psl\n",
    "import torchsde\n",
    "import torchsde\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from neuromancer.system import Node\n",
    "from neuromancer.dynamics import integrators, ode\n",
    "from neuromancer.trainer import Trainer, LitTrainer\n",
    "from neuromancer.problem import Problem\n",
    "from neuromancer.loggers import BasicLogger\n",
    "from neuromancer.dataset import DictDataset\n",
    "from neuromancer.constraint import variable\n",
    "from neuromancer.loss import PenaltyLoss\n",
    "from neuromancer.modules import blocks\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions import Normal\n",
    "from typing import Sequence\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to generate data from a Lorenz attractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearScheduler(object):\n",
    "    def __init__(self, iters, maxval=1.0):\n",
    "        self._iters = max(1, iters)\n",
    "        self._val = maxval / self._iters\n",
    "        self._maxval = maxval\n",
    "\n",
    "    def step(self):\n",
    "        self._val = min(self._maxval, self._val + self._maxval / self._iters)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val\n",
    "\n",
    "\n",
    "class StochasticLorenz(object):\n",
    "    \"\"\"Stochastic Lorenz attractor.\n",
    "\n",
    "    Used for simulating ground truth and obtaining noisy data.\n",
    "    Details described in Section 7.2 https://arxiv.org/pdf/2001.01328.pdf\n",
    "    Default a, b from https://openreview.net/pdf?id=HkzRQhR9YX\n",
    "    \"\"\"\n",
    "    noise_type = \"diagonal\"\n",
    "    sde_type = \"ito\"\n",
    "\n",
    "    def __init__(self, a: Sequence = (10., 28., 8 / 3), b: Sequence = (.1, .28, .3)):\n",
    "        super(StochasticLorenz, self).__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def f(self, t, y):\n",
    "        x1, x2, x3 = torch.split(y, split_size_or_sections=(1, 1, 1), dim=1)\n",
    "        a1, a2, a3 = self.a\n",
    "\n",
    "        f1 = a1 * (x2 - x1)\n",
    "        f2 = a2 * x1 - x2 - x1 * x3\n",
    "        f3 = x1 * x2 - a3 * x3\n",
    "        return torch.cat([f1, f2, f3], dim=1)\n",
    "\n",
    "    def g(self, t, y):\n",
    "        x1, x2, x3 = torch.split(y, split_size_or_sections=(1, 1, 1), dim=1)\n",
    "        b1, b2, b3 = self.b\n",
    "\n",
    "        g1 = x1 * b1\n",
    "        g2 = x2 * b2\n",
    "        g3 = x3 * b3\n",
    "        return torch.cat([g1, g2, g3], dim=1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, x0, ts, noise_std, normalize):\n",
    "        \"\"\"Sample data for training. Store data normalization constants if necessary.\"\"\"\n",
    "        xs = torchsde.sdeint(self, x0, ts)\n",
    "        if normalize:\n",
    "            mean, std = torch.mean(xs, dim=(0, 1)), torch.std(xs, dim=(0, 1))\n",
    "            xs.sub_(mean).div_(std).add_(torch.randn_like(xs) * noise_std)\n",
    "        return xs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuromancer Integration\n",
    "\n",
    "As per the NeuroMANCER x Lightning workflow, generate the data_setup_function and return the DictDatasets. Note that we only have a train dataset here, so we return `None` for dev/test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(t0, t1, batch_size, noise_std, steps=100):\n",
    "    _y0 = torch.randn(batch_size, 3)\n",
    "    ts = torch.linspace(t0, t1, steps=steps)\n",
    "    xs = StochasticLorenz().sample(_y0, ts, noise_std, normalize=True)\n",
    "    train_data = DictDataset({'xs':xs},name='train')\n",
    "    return train_data, None, None, batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some experimental parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=256\n",
    "latent_size=4\n",
    "context_size=64\n",
    "hidden_size=128\n",
    "lr_init=1e-2\n",
    "t0=0.\n",
    "t1=2.\n",
    "lr_gamma=0.997\n",
    "num_iters=1\n",
    "kl_anneal_iters=1000\n",
    "pause_every=50\n",
    "noise_std=0.01\n",
    "method=\"euler\"\n",
    "steps = 100 # number of time steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Neuromancer components, variables, and problem to train the LatentSDE. Upon training, this LatentSDE will generate new samples that exhibit the behavior of the Lorenz attractor training data. For this example, we set `adjoint` to `False` (do not use the adjoint sensitivity method). This is because this method seems to be significantly slower. \n",
    "\n",
    "Also note that we need to pass in the timestep tensor to our `LatentSDE_Encoder`, and as a result need to also define it outside the `make_dataset()` function. We note that this is not the cleanest code and breaks the data abstraction. Additional features will be added to mitigate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = torch.linspace(t0, t1, steps=steps)\n",
    "\n",
    "sde_block_encoder = blocks.LatentSDE_Encoder(3, latent_size, context_size, hidden_size, ts=ts, adjoint=False) \n",
    "integrator = integrators.LatentSDEIntegrator(sde_block_encoder, adjoint=False)\n",
    "model_1 = Node(integrator, input_keys=['xs'], output_keys=['zs', 'z0', 'log_ratio',  'xs', 'qz0_mean', 'qz0_logstd'], name='m1')\n",
    "sde_block_decoder = blocks.LatentSDE_Decoder(3, latent_size, noise_std=noise_std)\n",
    "model_2 = Node(sde_block_decoder, input_keys=['xs', 'zs', 'log_ratio', 'qz0_mean', 'qz0_logstd'], output_keys=['xs_hat', 'log_pxs', 'sum_term', 'log_ratio'], name='m2' )\n",
    "\n",
    "xs = variable('xs')\n",
    "zs = variable('zs')\n",
    "z0 = variable('z0')\n",
    "xs_hat = variable('xs_hat')\n",
    "\n",
    "\n",
    "log_ratio = variable('log_ratio')\n",
    "qz0_mean = variable('qz0_mean')\n",
    "qz0_logstd = variable('qz0_logstd')\n",
    "log_pxs = variable('log_pxs')\n",
    "sum_term = variable('sum_term')\n",
    "\n",
    "# NeuroMANCER loss function format\n",
    "loss = (-1.0*log_pxs + log_ratio) == 0.0\n",
    "\n",
    "# aggregate list of objective terms and constraints\n",
    "objectives = [loss]\n",
    "constraints = []\n",
    "# create constrained optimization loss\n",
    "loss = PenaltyLoss(objectives, constraints)\n",
    "# construct constrained optimization problem\n",
    "problem = Problem([model_1, model_2], loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define helper visualization function (again see https://github.com/google-research/torchsde/blob/master/examples/latent_sde_lorenz.py) that will fire every N epochs in our training loop. This visualization will allow us to see the learned Lorenz attractor samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the same Brownian motion for visualization.\n",
    "bm_vis = torchsde.BrownianInterval(\n",
    "    t0=t0, t1=t1, size=(batch_size, latent_size,), device='cpu', levy_area_approximation=\"space-time\")\n",
    "\n",
    "# \n",
    "def vis(data_dict, problem, bm_vis, img_path, num_samples=10):\n",
    "    encoder, decoder = problem.nodes[0], problem.nodes[1] #extract the encoder and decoder from our problem\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 9))\n",
    "    gs = gridspec.GridSpec(1, 2)\n",
    "    ax00 = fig.add_subplot(gs[0, 0], projection='3d')\n",
    "    ax01 = fig.add_subplot(gs[0, 1], projection='3d')\n",
    "\n",
    "    xs = data_dict['xs'] #pull out data sample from the DictDataset\n",
    "    # Left plot: data.\n",
    "    z1, z2, z3 = np.split(xs.cpu().numpy(), indices_or_sections=3, axis=-1)\n",
    "    [ax00.plot(z1[:, i, 0], z2[:, i, 0], z3[:, i, 0]) for i in range(num_samples)]\n",
    "    ax00.scatter(z1[0, :num_samples, 0], z2[0, :num_samples, 0], z3[0, :10, 0], marker='x')\n",
    "    ax00.set_yticklabels([])\n",
    "    ax00.set_xticklabels([])\n",
    "    ax00.set_zticklabels([])\n",
    "    ax00.set_xlabel('$z_1$', labelpad=0., fontsize=16)\n",
    "    ax00.set_ylabel('$z_2$', labelpad=.5, fontsize=16)\n",
    "    ax00.set_zlabel('$z_3$', labelpad=0., horizontalalignment='center', fontsize=16)\n",
    "    ax00.set_title('Data', fontsize=20)\n",
    "    xlim = ax00.get_xlim()\n",
    "    ylim = ax00.get_ylim()\n",
    "    zlim = ax00.get_zlim()\n",
    "\n",
    "    # Right plot: samples from learned model.\n",
    "    mydata = data_dict\n",
    "    output = decoder(encoder(mydata))\n",
    "    xs_hat = output['xs_hat'].detach().cpu().numpy() \n",
    "    #xs = latent_sde.sample(batch_size=xs.size(1), ts=ts, bm=bm_vis).cpu().numpy()\n",
    "    z1, z2, z3 = np.split(xs_hat, indices_or_sections=3, axis=-1)\n",
    "\n",
    "    [ax01.plot(z1[:, i, 0], z2[:, i, 0], z3[:, i, 0]) for i in range(num_samples)]\n",
    "    ax01.scatter(z1[0, :num_samples, 0], z2[0, :num_samples, 0], z3[0, :10, 0], marker='x')\n",
    "    ax01.set_yticklabels([])\n",
    "    ax01.set_xticklabels([])\n",
    "    ax01.set_zticklabels([])\n",
    "    ax01.set_xlabel('$z_1$', labelpad=0., fontsize=16)\n",
    "    ax01.set_ylabel('$z_2$', labelpad=.5, fontsize=16)\n",
    "    ax01.set_zlabel('$z_3$', labelpad=0., horizontalalignment='center', fontsize=16)\n",
    "    ax01.set_title('Samples', fontsize=20)\n",
    "    ax01.set_xlim(xlim)\n",
    "    ax01.set_ylim(ylim)\n",
    "    ax01.set_zlim(zlim)\n",
    "\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuromancer training the problem to learn the stochastic process\n",
    "\n",
    "We now train and visualize results using Lightning workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fix the same Brownian motion for visualization.\n",
    "bm_vis = torchsde.BrownianInterval(\n",
    "    t0=t0, t1=t1, size=(batch_size, latent_size,), device='cpu', levy_area_approximation=\"space-time\")\n",
    "\n",
    "# Define the custom_training_step to support visualization. \n",
    "def custom_training_step(model, batch): \n",
    "    output = model.problem(batch)\n",
    "    loss = output[model.train_metric]\n",
    "    img_path = os.path.join('', f'current_epoch_{model.current_epoch:06d}.pdf')\n",
    "    if model.current_epoch % 50 == 0: \n",
    "        vis(batch,  model.problem, bm_vis, img_path, num_samples=10)\n",
    "    return loss\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(problem.parameters(), lr=0.001)\n",
    "lit_trainer = LitTrainer(epochs=300, accelerator='cpu', train_metric='train_loss', \n",
    "                         dev_metric='train_loss', eval_metric='train_loss', test_metric='train_loss',\n",
    "                         custom_optimizer=optimizer, custom_training_step=custom_training_step)\n",
    "\n",
    "\n",
    "\n",
    "lit_trainer.fit(problem=problem, data_setup_function=make_dataset,  t0=t0, t1=t1, batch_size=batch_size, noise_std=noise_std)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuromancer3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
