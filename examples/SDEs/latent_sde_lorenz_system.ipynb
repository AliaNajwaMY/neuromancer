{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchSDE + Neuromancer: Latent Stochastic Differential Equations (System ID of Stochastic Process)\n",
    "\n",
    "This notebook goes over how to utilize torchsde's functionality within Neuromancer framework. This notebook is based off: https://github.com/google-research/torchsde/blob/master/examples/latent_sde_lorenz.py. In this example, we generate data according to a 3-dimensional stochastic Lorenz attractor. We then perform a \"system identification\" on this data -- seek to model a stochastic differential equation on this data. Upon performant training, this LatentSDE will then be able to reproduce samples that exhibit the same behavior as the provided Lorenz system. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f88287f00d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from neuromancer.psl import plot\n",
    "from neuromancer import psl\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from neuromancer.system import Node\n",
    "from neuromancer.dynamics import integrators, ode\n",
    "from neuromancer.trainer import Trainer, LitTrainer\n",
    "from neuromancer.problem import Problem\n",
    "from neuromancer.loggers import BasicLogger\n",
    "from neuromancer.dataset import DictDataset\n",
    "from neuromancer.constraint import variable\n",
    "from neuromancer.loss import PenaltyLoss\n",
    "from neuromancer.modules import blocks\n",
    "\n",
    "torch.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import Sequence\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import torchsde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to generate data from a Lorenz attractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearScheduler(object):\n",
    "    def __init__(self, iters, maxval=1.0):\n",
    "        self._iters = max(1, iters)\n",
    "        self._val = maxval / self._iters\n",
    "        self._maxval = maxval\n",
    "\n",
    "    def step(self):\n",
    "        self._val = min(self._maxval, self._val + self._maxval / self._iters)\n",
    "\n",
    "    @property\n",
    "    def val(self):\n",
    "        return self._val\n",
    "\n",
    "\n",
    "class StochasticLorenz(object):\n",
    "    \"\"\"Stochastic Lorenz attractor.\n",
    "\n",
    "    Used for simulating ground truth and obtaining noisy data.\n",
    "    Details described in Section 7.2 https://arxiv.org/pdf/2001.01328.pdf\n",
    "    Default a, b from https://openreview.net/pdf?id=HkzRQhR9YX\n",
    "    \"\"\"\n",
    "    noise_type = \"diagonal\"\n",
    "    sde_type = \"ito\"\n",
    "\n",
    "    def __init__(self, a: Sequence = (10., 28., 8 / 3), b: Sequence = (.1, .28, .3)):\n",
    "        super(StochasticLorenz, self).__init__()\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "    def f(self, t, y):\n",
    "        x1, x2, x3 = torch.split(y, split_size_or_sections=(1, 1, 1), dim=1)\n",
    "        a1, a2, a3 = self.a\n",
    "\n",
    "        f1 = a1 * (x2 - x1)\n",
    "        f2 = a2 * x1 - x2 - x1 * x3\n",
    "        f3 = x1 * x2 - a3 * x3\n",
    "        return torch.cat([f1, f2, f3], dim=1)\n",
    "\n",
    "    def g(self, t, y):\n",
    "        x1, x2, x3 = torch.split(y, split_size_or_sections=(1, 1, 1), dim=1)\n",
    "        b1, b2, b3 = self.b\n",
    "\n",
    "        g1 = x1 * b1\n",
    "        g2 = x2 * b2\n",
    "        g3 = x3 * b3\n",
    "        return torch.cat([g1, g2, g3], dim=1)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, x0, ts, noise_std, normalize):\n",
    "        \"\"\"Sample data for training. Store data normalization constants if necessary.\"\"\"\n",
    "        xs = torchsde.sdeint(self, x0, ts)\n",
    "        if normalize:\n",
    "            mean, std = torch.mean(xs, dim=(0, 1)), torch.std(xs, dim=(0, 1))\n",
    "            xs.sub_(mean).div_(std).add_(torch.randn_like(xs) * noise_std)\n",
    "        return xs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vis(xs, ts, latent_sde, bm_vis, img_path, num_samples=10):\n",
    "    fig = plt.figure(figsize=(20, 9))\n",
    "    gs = gridspec.GridSpec(1, 2)\n",
    "    ax00 = fig.add_subplot(gs[0, 0], projection='3d')\n",
    "    ax01 = fig.add_subplot(gs[0, 1], projection='3d')\n",
    "\n",
    "    # Left plot: data.\n",
    "    z1, z2, z3 = np.split(xs.cpu().numpy(), indices_or_sections=3, axis=-1)\n",
    "    [ax00.plot(z1[:, i, 0], z2[:, i, 0], z3[:, i, 0]) for i in range(num_samples)]\n",
    "    ax00.scatter(z1[0, :num_samples, 0], z2[0, :num_samples, 0], z3[0, :10, 0], marker='x')\n",
    "    ax00.set_yticklabels([])\n",
    "    ax00.set_xticklabels([])\n",
    "    ax00.set_zticklabels([])\n",
    "    ax00.set_xlabel('$z_1$', labelpad=0., fontsize=16)\n",
    "    ax00.set_ylabel('$z_2$', labelpad=.5, fontsize=16)\n",
    "    ax00.set_zlabel('$z_3$', labelpad=0., horizontalalignment='center', fontsize=16)\n",
    "    ax00.set_title('Data', fontsize=20)\n",
    "    xlim = ax00.get_xlim()\n",
    "    ylim = ax00.get_ylim()\n",
    "    zlim = ax00.get_zlim()\n",
    "\n",
    "    # Right plot: samples from learned model.\n",
    "    xs = latent_sde.sample(batch_size=xs.size(1), ts=ts, bm=bm_vis).cpu().numpy()\n",
    "    z1, z2, z3 = np.split(xs, indices_or_sections=3, axis=-1)\n",
    "\n",
    "    [ax01.plot(z1[:, i, 0], z2[:, i, 0], z3[:, i, 0]) for i in range(num_samples)]\n",
    "    ax01.scatter(z1[0, :num_samples, 0], z2[0, :num_samples, 0], z3[0, :10, 0], marker='x')\n",
    "    ax01.set_yticklabels([])\n",
    "    ax01.set_xticklabels([])\n",
    "    ax01.set_zticklabels([])\n",
    "    ax01.set_xlabel('$z_1$', labelpad=0., fontsize=16)\n",
    "    ax01.set_ylabel('$z_2$', labelpad=.5, fontsize=16)\n",
    "    ax01.set_zlabel('$z_3$', labelpad=0., horizontalalignment='center', fontsize=16)\n",
    "    ax01.set_title('Samples', fontsize=20)\n",
    "    ax01.set_xlim(xlim)\n",
    "    ax01.set_ylim(ylim)\n",
    "    ax01.set_zlim(zlim)\n",
    "\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1024\n",
    "latent_size=4\n",
    "context_size=64\n",
    "hidden_size=128\n",
    "lr_init=1e-2\n",
    "t0=0.\n",
    "t1=2.\n",
    "lr_gamma=0.997\n",
    "num_iters=1\n",
    "kl_anneal_iters=1000\n",
    "pause_every=50\n",
    "noise_std=0.01\n",
    "adjoint=False\n",
    "train_dir='./dump/lorenz/'\n",
    "method=\"euler\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuromancer Integration\n",
    "\n",
    "Generate the data and create Neuromancer DictDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(t0, t1, batch_size, noise_std):\n",
    "    _y0 = torch.randn(batch_size, 3)\n",
    "    ts = torch.linspace(t0, t1, steps=100)\n",
    "    xs = StochasticLorenz().sample(_y0, ts, noise_std, normalize=True)\n",
    "    train_data = DictDataset({'xs':xs},name='train')\n",
    "    dev_data = DictDataset({'xs':xs},name='dev')\n",
    "    test_data = DictDataset({'xs':xs},name='test')\n",
    "    return train_data, None, None, batch_size\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Neuromancer components, variables, and problem to train the LatentSDE. Upon training, this LatentSDE will generate new samples that exhibit the behavior of the Lorenz attractor training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "torch.manual_seed(0)\n",
    "batch_size=1024\n",
    "latent_size=4\n",
    "context_size=64\n",
    "hidden_size=128\n",
    "lr_init=1e-2\n",
    "t0=0.\n",
    "t1=2.\n",
    "lr_gamma=0.997\n",
    "num_iters=1\n",
    "kl_anneal_iters=1000\n",
    "pause_every=50\n",
    "noise_std=0.01\n",
    "adjoint=False\n",
    "train_dir='./dump/lorenz/'\n",
    "method=\"euler\"\n",
    "ts = torch.linspace(t0, t1, steps=100)\n",
    "\n",
    "sde_block_encoder = blocks.LatentSDE_Encoder(3, latent_size, context_size, hidden_size, ts=ts, adjoint=True) \n",
    "integrator = integrators.LatentSDEIntegrator(sde_block_encoder, adjoint=True)\n",
    "model_1 = Node(integrator, input_keys=['xs'], output_keys=['zs', 'z0', 'log_ratio',  'xs', 'qz0_mean', 'qz0_logstd'], name='m1')\n",
    "sde_block_decoder = blocks.LatentSDE_Decoder(3, latent_size, noise_std=noise_std)\n",
    "model_2 = Node(sde_block_decoder, input_keys=['xs', 'zs', 'log_ratio', 'qz0_mean', 'qz0_logstd'], output_keys=['xs_hat', 'log_pxs', 'sum_term', 'log_ratio'], name='m2' )\n",
    "\n",
    "xs = variable('xs')\n",
    "zs = variable('zs')\n",
    "z0 = variable('z0')\n",
    "xs_hat = variable('xs_hat')\n",
    "\n",
    "\n",
    "log_ratio = variable('log_ratio')\n",
    "qz0_mean = variable('qz0_mean')\n",
    "qz0_logstd = variable('qz0_logstd')\n",
    "log_pxs = variable('log_pxs')\n",
    "sum_term = variable('sum_term')\n",
    "\n",
    "\n",
    "\n",
    "loss = (-1.0*log_pxs + log_ratio) == 0.0\n",
    "\n",
    "\n",
    "# aggregate list of objective terms and constraints\n",
    "objectives = [loss]\n",
    "constraints = []\n",
    "# create constrained optimization loss\n",
    "loss = PenaltyLoss(objectives, constraints)\n",
    "# construct constrained optimization problem\n",
    "problem = Problem([model_1, model_2], loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the same Brownian motion for visualization.\n",
    "bm_vis = torchsde.BrownianInterval(\n",
    "    t0=t0, t1=t1, size=(batch_size, latent_size,), device='cpu', levy_area_approximation=\"space-time\")\n",
    "\n",
    "def vis(xs, ts, problem, bm_vis, img_path, num_samples=10):\n",
    "    fig = plt.figure(figsize=(20, 9))\n",
    "    gs = gridspec.GridSpec(1, 2)\n",
    "    ax00 = fig.add_subplot(gs[0, 0], projection='3d')\n",
    "    ax01 = fig.add_subplot(gs[0, 1], projection='3d')\n",
    "\n",
    "    # Left plot: data.\n",
    "    z1, z2, z3 = np.split(xs.cpu().numpy(), indices_or_sections=3, axis=-1)\n",
    "    [ax00.plot(z1[:, i, 0], z2[:, i, 0], z3[:, i, 0]) for i in range(num_samples)]\n",
    "    ax00.scatter(z1[0, :num_samples, 0], z2[0, :num_samples, 0], z3[0, :10, 0], marker='x')\n",
    "    ax00.set_yticklabels([])\n",
    "    ax00.set_xticklabels([])\n",
    "    ax00.set_zticklabels([])\n",
    "    ax00.set_xlabel('$z_1$', labelpad=0., fontsize=16)\n",
    "    ax00.set_ylabel('$z_2$', labelpad=.5, fontsize=16)\n",
    "    ax00.set_zlabel('$z_3$', labelpad=0., horizontalalignment='center', fontsize=16)\n",
    "    ax00.set_title('Data', fontsize=20)\n",
    "    xlim = ax00.get_xlim()\n",
    "    ylim = ax00.get_ylim()\n",
    "    zlim = ax00.get_zlim()\n",
    "\n",
    "    # Right plot: samples from learned model.\n",
    "    xs = problem\n",
    "    xs = latent_sde.sample(batch_size=xs.size(1), ts=ts, bm=bm_vis).cpu().numpy()\n",
    "    z1, z2, z3 = np.split(xs, indices_or_sections=3, axis=-1)\n",
    "\n",
    "    [ax01.plot(z1[:, i, 0], z2[:, i, 0], z3[:, i, 0]) for i in range(num_samples)]\n",
    "    ax01.scatter(z1[0, :num_samples, 0], z2[0, :num_samples, 0], z3[0, :10, 0], marker='x')\n",
    "    ax01.set_yticklabels([])\n",
    "    ax01.set_xticklabels([])\n",
    "    ax01.set_zticklabels([])\n",
    "    ax01.set_xlabel('$z_1$', labelpad=0., fontsize=16)\n",
    "    ax01.set_ylabel('$z_2$', labelpad=.5, fontsize=16)\n",
    "    ax01.set_zlabel('$z_3$', labelpad=0., horizontalalignment='center', fontsize=16)\n",
    "    ax01.set_title('Samples', fontsize=20)\n",
    "    ax01.set_xlim(xlim)\n",
    "    ax01.set_ylim(ylim)\n",
    "    ax01.set_zlim(zlim)\n",
    "\n",
    "    plt.savefig(img_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuromancer training the problem to learn the stochastic process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/birm560/opt/anaconda3/envs/neuromancer8/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:653: Checkpoint directory /Users/birm560/Library/CloudStorage/OneDrive-PNNL/Documents/neuromancer/neuromancer/examples/SDEs exists and is not empty.\n",
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | problem | Problem | 104 K \n",
      "------------------------------------\n",
      "104 K     Trainable params\n",
      "0         Non-trainable params\n",
      "104 K     Total params\n",
      "0.420     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USING BATCH SIZE  1024\n",
      "USING LEARNING RATE  0.001\n",
      "                                                  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/birm560/opt/anaconda3/envs/neuromancer8/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/birm560/opt/anaconda3/envs/neuromancer8/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:104: Total length of `DataLoader` across ranks is zero. Please make sure this was your intention.\n",
      "/Users/birm560/opt/anaconda3/envs/neuromancer8/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=15` in the `DataLoader` to improve performance.\n",
      "/Users/birm560/opt/anaconda3/envs/neuromancer8/lib/python3.10/site-packages/lightning/pytorch/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/1 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/birm560/Library/CloudStorage/OneDrive-PNNL/Documents/neuromancer/neuromancer/src/neuromancer/constraint.py:169: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([99, 1024])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.l1_loss(left, right)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 1/1 [00:46<00:00,  0.02it/s, v_num=5, train_loss_step=1.910, train_loss_epoch=1.910]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 1: 'train_loss' reached 1.90976 (best 1.90976), saving model to '/Users/birm560/Library/CloudStorage/OneDrive-PNNL/Documents/neuromancer/neuromancer/examples/SDEs/epoch=0-step=1.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1/1 [00:44<00:00,  0.02it/s, v_num=5, train_loss_step=0.360, train_loss_epoch=0.360]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 2: 'train_loss' reached 0.35969 (best 0.35969), saving model to '/Users/birm560/Library/CloudStorage/OneDrive-PNNL/Documents/neuromancer/neuromancer/examples/SDEs/epoch=1-step=2.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:   0%|          | 0/1 [00:00<?, ?it/s, v_num=5, train_loss_step=0.360, train_loss_epoch=0.360]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/birm560/opt/anaconda3/envs/neuromancer8/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.Adam(problem.parameters(), lr=0.001)\n",
    "lit_trainer = LitTrainer(epochs=30, accelerator='cpu', train_metric='train_loss', \n",
    "                         dev_metric='train_loss', eval_metric='train_loss', test_metric='train_loss',\n",
    "                         custom_optimizer=optimizer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lit_trainer.fit(problem=problem, data_setup_function=make_dataset, t0=t0, t1=t1, batch_size=batch_size, noise_std=noise_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): \n",
    "    dataz = \n",
    "    output = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Learned Stochastic Samples\n",
    "\n",
    "*TODO*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuromancer3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
